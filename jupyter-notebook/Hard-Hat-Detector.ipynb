{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Hat Detector\n",
    "\n",
    "### Initialize the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental.preprocessing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a95de5c3b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimageai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mObjectDetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/imageai/Detection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimageai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_retinanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet50_retinanet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimageai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_retinanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_image_bgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_image_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_image_stream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimageai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_retinanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_caption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/imageai/Detection/keras_retinanet/models/resnet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimageai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_resnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimageai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_resnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     raise ImportError(\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "from imageai.Detection import ObjectDetection\n",
    "\n",
    "import numpy as np\n",
    "import requests as req\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(img):\n",
    "    window_name = 'image'\n",
    "    cv.imshow(window_name, img)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "### Download Images of Hard Hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardhatLoc = 'http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n03492922'\n",
    "\n",
    "hardhatImages = req.get(hardhatLoc).text\n",
    "noOfImages = 0\n",
    "\n",
    "if not os.path.exists('hardhat'):\n",
    "    os.makedirs('hardhat')\n",
    "\n",
    "for i in hardhatImages.split('\\n'):\n",
    "    try:\n",
    "        r = req.get(i, timeout=0.5)\n",
    "        file = i.split(\"/\")[-1].split('\\r')[0]\n",
    "        if 'image/jpeg' in r.headers['Content-Type']:\n",
    "            if len(r.content) > 8192:\n",
    "                with open('hardhat\\\\' + file, 'wb') as outfile:\n",
    "                    outfile.write(r.content)\n",
    "                    noOfImages += 1\n",
    "                    print('Success: ' + file)\n",
    "            else:\n",
    "                print('Failed: ' + file + ' -- Image too small')\n",
    "        else:\n",
    "            print('Failed: ' + file + ' -- Not an image')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Failed: ' + file + ' -- Error')\n",
    "        \n",
    "print('*********** Download Finished **************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Images of People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleLoc = 'http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07942152'\n",
    "\n",
    "peopleImages = req.get(peopleLoc).text\n",
    "noOfImages = 0\n",
    "\n",
    "if not os.path.exists('people'):\n",
    "    os.makedirs('people')\n",
    "\n",
    "for i in peopleImages.split('\\n'):\n",
    "    try:\n",
    "        r = req.get(i, timeout=0.5)\n",
    "        file = i.split(\"/\")[-1].split('\\r')[0]\n",
    "        if 'image/jpeg' in r.headers['Content-Type']:\n",
    "            if len(r.content) > 8192:\n",
    "                with open('people\\\\' + file, 'wb') as outfile:\n",
    "                    outfile.write(r.content)\n",
    "                    noOfImages += 1\n",
    "                    print('Success: ' + file)\n",
    "            else:\n",
    "                print('Failed: ' + file + ' -- Image too small')\n",
    "        else:\n",
    "            print('Failed: ' + file + ' -- Not an image')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Failed: ' + file + ' -- Error')\n",
    "        \n",
    "print('*********** Download Finished **************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pre-Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRetinaNet = 'https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5'\n",
    "modelYOLOv3 = 'https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5'\n",
    "modelTinyYOLOv3 = 'https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5'\n",
    "\n",
    "if not os.path.exists('yolo.h5'):\n",
    "    r = req.get(modelYOLOv3, timeout=0.5)\n",
    "    with open('yolo.h5', 'wb') as outfile:\n",
    "        outfile.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "### Define the Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From E:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath('yolo.h5')\n",
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Ensure you specified correct input image, input type, output type and/or output image path ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\imageai\\Detection\\__init__.py\u001b[0m in \u001b[0;36mdetectCustomObjectsFromImage\u001b[1;34m(self, custom_objects, input_image, output_image_path, input_type, output_type, extract_detected_objects, minimum_percentage_probability, display_percentage_probability, display_object_name, thread_safe)\u001b[0m\n\u001b[0;32m    795\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m                         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m                         \u001b[0minput_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_image_bgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2842\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2843\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'hardhat/test'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6d8480a8c66a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     detectedImage, detections = detector.detectCustomObjectsFromImage(custom_objects=peopleOnly, output_type=\"array\",\n\u001b[0;32m      7\u001b[0m                                                                       \u001b[0minput_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimageFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                                                       minimum_percentage_probability=30)\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"hardhat-clean/{0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\imageai\\Detection\\__init__.py\u001b[0m in \u001b[0;36mdetectCustomObjectsFromImage\u001b[1;34m(self, custom_objects, input_image, output_image_path, input_type, output_type, extract_detected_objects, minimum_percentage_probability, display_percentage_probability, display_object_name, thread_safe)\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m                 raise ValueError(\n\u001b[1;32m--> 919\u001b[1;33m                     \"Ensure you specified correct input image, input type, output type and/or output image path \")\n\u001b[0m\u001b[0;32m    920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Ensure you specified correct input image, input type, output type and/or output image path "
     ]
    }
   ],
   "source": [
    "hardhatImages = os.listdir(\"hardhat\")\n",
    "peopleOnly = detector.CustomObjects(person=True)\n",
    "\n",
    "for i in hardhatImages:\n",
    "    imageFile = \"hardhat/{0}\".format(i)\n",
    "    detectedImage, detections = detector.detectCustomObjectsFromImage(custom_objects=peopleOnly, output_type=\"array\",\n",
    "                                                                      input_image=imageFile, \n",
    "                                                                      minimum_percentage_probability=30)\n",
    "    if len(detections) < 0:\n",
    "        os.remove(imageFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('hardhat/train/images'):\n",
    "    os.makedirs('hardhat/train/images')\n",
    "if not os.path.exists('hardhat/validation/images'):\n",
    "    os.makedirs('hardhat/validation/images')\n",
    "\n",
    "hardhatImages = os.listdir(\"hardhat\")\n",
    "hardhatTrainNums = round(len(hardhatImages) * 0.90)\n",
    "\n",
    "for i in range(0, hardhatTrainNums):\n",
    "    file = \"hardhat/\" + hardhatImages[i]\n",
    "    if os.path.isfile(file):\n",
    "        os.rename(file, \"hardhat/train/images/\" + hardhatImages[i])\n",
    "    \n",
    "hardhatImages = os.listdir(\"hardhat\")\n",
    "\n",
    "for i in hardhatImages:\n",
    "    file = \"hardhat/\" + i\n",
    "    if os.path.isfile(file):\n",
    "        os.rename(file, \"hardhat/validation/images/\" + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor boxes for training images and annotation...\n",
      "Average IOU for 9 anchors: 0.75\n",
      "Anchor Boxes generated.\n",
      "Detection configuration saved in  hardhat\\json\\detection_config.json\n",
      "Training on: \t['person hardhat']\n",
      "Training with Batch Size:  4\n",
      "Number of Experiments:  200\n",
      "Training with transfer learning from pretrained Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\keras\\callbacks\\callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "944/944 [==============================] - 856s 907ms/step - loss: 36.9858 - yolo_layer_16_loss: 7.0103 - yolo_layer_17_loss: 10.9458 - yolo_layer_18_loss: 19.0297 - val_loss: 22.8127 - val_yolo_layer_16_loss: 5.6805 - val_yolo_layer_17_loss: 6.6321 - val_yolo_layer_18_loss: 12.3109\n",
      "WARNING:tensorflow:From E:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/200\n",
      "944/944 [==============================] - 774s 820ms/step - loss: 20.2127 - yolo_layer_16_loss: 4.6473 - yolo_layer_17_loss: 5.6500 - yolo_layer_18_loss: 9.9154 - val_loss: 12.3263 - val_yolo_layer_16_loss: 5.5798 - val_yolo_layer_17_loss: 6.8109 - val_yolo_layer_18_loss: 11.0726\n",
      "Epoch 3/200\n",
      "944/944 [==============================] - 776s 822ms/step - loss: 18.3042 - yolo_layer_16_loss: 4.7082 - yolo_layer_17_loss: 4.9417 - yolo_layer_18_loss: 8.6543 - val_loss: 30.1679 - val_yolo_layer_16_loss: 3.5330 - val_yolo_layer_17_loss: 5.7483 - val_yolo_layer_18_loss: 10.3959\n",
      "Epoch 4/200\n",
      "944/944 [==============================] - 772s 817ms/step - loss: 15.8019 - yolo_layer_16_loss: 3.5019 - yolo_layer_17_loss: 4.2786 - yolo_layer_18_loss: 8.0214 - val_loss: 25.0318 - val_yolo_layer_16_loss: 3.3268 - val_yolo_layer_17_loss: 4.8476 - val_yolo_layer_18_loss: 8.9261\n",
      "Epoch 5/200\n",
      "944/944 [==============================] - 775s 821ms/step - loss: 14.2383 - yolo_layer_16_loss: 2.8789 - yolo_layer_17_loss: 3.9230 - yolo_layer_18_loss: 7.4365 - val_loss: 20.3871 - val_yolo_layer_16_loss: 2.7601 - val_yolo_layer_17_loss: 4.3188 - val_yolo_layer_18_loss: 8.2737\n",
      "Epoch 6/200\n",
      "944/944 [==============================] - 774s 820ms/step - loss: 13.3032 - yolo_layer_16_loss: 2.6941 - yolo_layer_17_loss: 3.6415 - yolo_layer_18_loss: 6.9677 - val_loss: 12.4483 - val_yolo_layer_16_loss: 2.4334 - val_yolo_layer_17_loss: 4.4343 - val_yolo_layer_18_loss: 7.3737\n",
      "Epoch 7/200\n",
      "944/944 [==============================] - 773s 819ms/step - loss: 12.7051 - yolo_layer_16_loss: 2.4235 - yolo_layer_17_loss: 3.3868 - yolo_layer_18_loss: 6.8948 - val_loss: 13.0070 - val_yolo_layer_16_loss: 2.1589 - val_yolo_layer_17_loss: 4.2864 - val_yolo_layer_18_loss: 7.7522\n",
      "Epoch 8/200\n",
      "944/944 [==============================] - 774s 820ms/step - loss: 12.0266 - yolo_layer_16_loss: 2.3517 - yolo_layer_17_loss: 3.1444 - yolo_layer_18_loss: 6.5304 - val_loss: 10.2641 - val_yolo_layer_16_loss: 2.4435 - val_yolo_layer_17_loss: 3.8413 - val_yolo_layer_18_loss: 7.1583\n",
      "Epoch 9/200\n",
      "944/944 [==============================] - 774s 819ms/step - loss: 11.5796 - yolo_layer_16_loss: 2.0791 - yolo_layer_17_loss: 3.0983 - yolo_layer_18_loss: 6.4023 - val_loss: 16.9550 - val_yolo_layer_16_loss: 1.9408 - val_yolo_layer_17_loss: 3.7394 - val_yolo_layer_18_loss: 8.5159\n",
      "Epoch 10/200\n",
      "944/944 [==============================] - 773s 819ms/step - loss: 11.0530 - yolo_layer_16_loss: 2.0455 - yolo_layer_17_loss: 2.9342 - yolo_layer_18_loss: 6.0733 - val_loss: 11.9410 - val_yolo_layer_16_loss: 2.4114 - val_yolo_layer_17_loss: 3.6174 - val_yolo_layer_18_loss: 7.4209\n",
      "Epoch 11/200\n",
      "944/944 [==============================] - 776s 822ms/step - loss: 10.6817 - yolo_layer_16_loss: 1.9981 - yolo_layer_17_loss: 2.8343 - yolo_layer_18_loss: 5.8493 - val_loss: 7.0664 - val_yolo_layer_16_loss: 1.9985 - val_yolo_layer_17_loss: 3.6278 - val_yolo_layer_18_loss: 6.6847\n",
      "Epoch 12/200\n",
      "944/944 [==============================] - 774s 820ms/step - loss: 10.4226 - yolo_layer_16_loss: 1.9022 - yolo_layer_17_loss: 2.7187 - yolo_layer_18_loss: 5.8017 - val_loss: 13.9414 - val_yolo_layer_16_loss: 2.1432 - val_yolo_layer_17_loss: 3.8370 - val_yolo_layer_18_loss: 6.8831\n",
      "Epoch 13/200\n",
      "944/944 [==============================] - 775s 821ms/step - loss: 10.0310 - yolo_layer_16_loss: 1.8245 - yolo_layer_17_loss: 2.6859 - yolo_layer_18_loss: 5.5206 - val_loss: 11.2866 - val_yolo_layer_16_loss: 1.8409 - val_yolo_layer_17_loss: 3.3783 - val_yolo_layer_18_loss: 6.5706\n",
      "Epoch 14/200\n",
      "944/944 [==============================] - 775s 821ms/step - loss: 9.7983 - yolo_layer_16_loss: 1.7165 - yolo_layer_17_loss: 2.5344 - yolo_layer_18_loss: 5.5474 - val_loss: 15.4132 - val_yolo_layer_16_loss: 1.7203 - val_yolo_layer_17_loss: 3.8468 - val_yolo_layer_18_loss: 6.7124\n",
      "Epoch 15/200\n",
      "944/944 [==============================] - 774s 820ms/step - loss: 9.6199 - yolo_layer_16_loss: 1.6960 - yolo_layer_17_loss: 2.5321 - yolo_layer_18_loss: 5.3918 - val_loss: 11.7558 - val_yolo_layer_16_loss: 1.6791 - val_yolo_layer_17_loss: 3.3249 - val_yolo_layer_18_loss: 6.7528\n",
      "Epoch 16/200\n",
      "944/944 [==============================] - 778s 824ms/step - loss: 9.2273 - yolo_layer_16_loss: 1.6217 - yolo_layer_17_loss: 2.3432 - yolo_layer_18_loss: 5.2624 - val_loss: 17.8903 - val_yolo_layer_16_loss: 1.8734 - val_yolo_layer_17_loss: 3.7065 - val_yolo_layer_18_loss: 6.7896\n",
      "Epoch 17/200\n",
      "944/944 [==============================] - 778s 825ms/step - loss: 9.0276 - yolo_layer_16_loss: 1.5986 - yolo_layer_17_loss: 2.3144 - yolo_layer_18_loss: 5.1147 - val_loss: 8.6253 - val_yolo_layer_16_loss: 1.6680 - val_yolo_layer_17_loss: 3.7442 - val_yolo_layer_18_loss: 5.6655\n",
      "Epoch 18/200\n",
      "944/944 [==============================] - 780s 827ms/step - loss: 8.8141 - yolo_layer_16_loss: 1.5611 - yolo_layer_17_loss: 2.3312 - yolo_layer_18_loss: 4.9219 - val_loss: 6.8751 - val_yolo_layer_16_loss: 1.4509 - val_yolo_layer_17_loss: 3.2254 - val_yolo_layer_18_loss: 6.5645\n",
      "Epoch 19/200\n",
      "944/944 [==============================] - 779s 825ms/step - loss: 8.6509 - yolo_layer_16_loss: 1.5625 - yolo_layer_17_loss: 2.2129 - yolo_layer_18_loss: 4.8754 - val_loss: 4.2779 - val_yolo_layer_16_loss: 1.4135 - val_yolo_layer_17_loss: 3.0607 - val_yolo_layer_18_loss: 6.1202\n",
      "Epoch 20/200\n",
      "944/944 [==============================] - 812s 860ms/step - loss: 8.4625 - yolo_layer_16_loss: 1.4308 - yolo_layer_17_loss: 2.1983 - yolo_layer_18_loss: 4.8334 - val_loss: 19.4190 - val_yolo_layer_16_loss: 1.4224 - val_yolo_layer_17_loss: 3.5843 - val_yolo_layer_18_loss: 7.1598\n",
      "Epoch 21/200\n",
      "944/944 [==============================] - 824s 872ms/step - loss: 8.3525 - yolo_layer_16_loss: 1.4480 - yolo_layer_17_loss: 2.2054 - yolo_layer_18_loss: 4.6992 - val_loss: 14.1312 - val_yolo_layer_16_loss: 1.6272 - val_yolo_layer_17_loss: 2.7905 - val_yolo_layer_18_loss: 5.9421\n",
      "Epoch 22/200\n",
      "682/944 [====================>.........] - ETA: 3:37 - loss: 8.1493 - yolo_layer_16_loss: 1.3431 - yolo_layer_17_loss: 2.0674 - yolo_layer_18_loss: 4.7388"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-cdca61b8cf02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                        train_from_pretrained_model=\"yolo.h5\")\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\imageai\\Detection\\Custom\\__init__.py\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m         )\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mE:\\Development\\Anaconda\\envs\\ImageAI\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from imageai.Detection.Custom import DetectionModelTrainer\n",
    "\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=\"hardhat\")\n",
    "\n",
    "trainer.setTrainConfig(object_names_array=[\"person hardhat\"], batch_size=4, num_experiments=200, \n",
    "                       train_from_pretrained_model=\"yolo.h5\")\n",
    "\n",
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model evaluation....\n",
      "Model File:  hardhat\\models\\detection_model-ex-005--loss-0014.238.h5 \n",
      "\n",
      "Using IoU :  0.5\n",
      "Using Object Threshold :  0.3\n",
      "Using Non-Maximum Suppression :  0.5\n",
      "person hardhat: 0.7165\n",
      "mAP: 0.7165\n",
      "===============================\n",
      "Starting Model evaluation....\n",
      "Model File:  hardhat\\models\\detection_model-ex-010--loss-0011.053.h5 \n",
      "\n",
      "Using IoU :  0.5\n",
      "Using Object Threshold :  0.3\n",
      "Using Non-Maximum Suppression :  0.5\n",
      "person hardhat: 0.7865\n",
      "mAP: 0.7865\n",
      "===============================\n",
      "Starting Model evaluation....\n",
      "Model File:  hardhat\\models\\detection_model-ex-015--loss-0009.620.h5 \n",
      "\n",
      "Using IoU :  0.5\n",
      "Using Object Threshold :  0.3\n",
      "Using Non-Maximum Suppression :  0.5\n",
      "person hardhat: 0.8355\n",
      "mAP: 0.8355\n",
      "===============================\n",
      "Starting Model evaluation....\n",
      "Model File:  hardhat\\models\\detection_model-ex-020--loss-0008.462.h5 \n",
      "\n",
      "Using IoU :  0.5\n",
      "Using Object Threshold :  0.3\n",
      "Using Non-Maximum Suppression :  0.5\n",
      "person hardhat: 0.8446\n",
      "mAP: 0.8446\n",
      "===============================\n",
      "---------------------------------------------------------\n",
      "Iteration 05: 0.7164811838244572 Iteration 10: 0.786450228152799 Iteration 15: 0.8354553845774733 Iteration 20: 0.8446486610895216\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model05 = trainer.evaluateModel(model_path=\"hardhat\\models\\detection_model-ex-005--loss-0014.238.h5\", \n",
    "                      json_path=\"hardhat\\json\\detection_config.json\", iou_threshold=0.5, \n",
    "                      object_threshold=0.3, nms_threshold=0.5)\n",
    "model10 = trainer.evaluateModel(model_path=\"hardhat\\models\\detection_model-ex-010--loss-0011.053.h5\", \n",
    "                      json_path=\"hardhat\\json\\detection_config.json\", iou_threshold=0.5, \n",
    "                      object_threshold=0.3, nms_threshold=0.5)\n",
    "model15 = trainer.evaluateModel(model_path=\"hardhat\\models\\detection_model-ex-015--loss-0009.620.h5\", \n",
    "                      json_path=\"hardhat\\json\\detection_config.json\", iou_threshold=0.5, \n",
    "                      object_threshold=0.3, nms_threshold=0.5)\n",
    "model20 = trainer.evaluateModel(model_path=\"hardhat\\models\\detection_model-ex-020--loss-0008.462.h5\", \n",
    "                      json_path=\"hardhat\\json\\detection_config.json\", iou_threshold=0.5, \n",
    "                      object_threshold=0.3, nms_threshold=0.5)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print('Iteration 05:', model05[0]['average_precision']['person hardhat'],\n",
    "     'Iteration 10:', model10[0]['average_precision']['person hardhat'],\n",
    "     'Iteration 15:', model15[0]['average_precision']['person hardhat'],\n",
    "     'Iteration 20:', model20[0]['average_precision']['person hardhat'])\n",
    "print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Hard Hat Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person hardhat  :  48.9349365234375  :  [198, 16, 253, 127]\n",
      "--------------------------------\n",
      "person hardhat  :  92.02308058738708  :  [84, 71, 127, 114]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "from imageai.Detection.Custom import CustomObjectDetection\n",
    "\n",
    "detector = CustomObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"hardhat\\models\\detection_model-ex-020--loss-0008.462.h5\")\n",
    "detector.setJsonPath(\"hardhat\\json\\detection_config.json\")\n",
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person hardhat  :  81.50655627250671  :  [137, 74, 194, 119]\n",
      "--------------------------------\n",
      "person hardhat  :  64.44770097732544  :  [290, 89, 328, 123]\n",
      "--------------------------------\n",
      "person hardhat  :  70.66482901573181  :  [252, 90, 290, 130]\n",
      "--------------------------------\n",
      "person hardhat  :  54.37401533126831  :  [358, 93, 385, 131]\n",
      "--------------------------------\n",
      "person hardhat  :  59.80772376060486  :  [125, 100, 145, 136]\n",
      "--------------------------------\n",
      "person hardhat  :  45.671346783638  :  [222, 101, 249, 130]\n",
      "--------------------------------\n",
      "person hardhat  :  77.1479070186615  :  [314, 99, 361, 142]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "testImages = os.listdir(\"hardhat/validation/images\")\n",
    "randomFile = testImages[random.randint(0, len(testImages) - 1)]\n",
    "\n",
    "detectedImage, detections = detector.detectObjectsFromImage(output_type=\"array\", \n",
    "                                                            input_image=\"hardhat/validation/images/{0}\".format(randomFile), \n",
    "                                                            minimum_percentage_probability=30)\n",
    "showImage(detectedImage)\n",
    "\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
